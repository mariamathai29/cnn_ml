# -*- coding: utf-8 -*-
"""Team2 - Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v9IO1tySnEK69PVgpufbAPZ8bUHsWpjd
"""

# Importing libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

import matplotlib.pyplot as plt # for plotting
import numpy as np
import torch.optim as optim #for gradient descent

torch.manual_seed(1) # set the random seed

print('Libraries Imported')

# Mounting your google drive to access the images
from google.colab import drive
drive.mount('/content/gdrive')

# Making dataloaders for training, testing and validation
base_path = '/content/gdrive/Shareddrives/GEC2023/Students/Team2/1'#EDIT YOUR PATH HERE (up till in dataset in gdrive)

# Transform Settings - Do not use RandomResizedCrop
transform = transforms.Compose([transforms.Resize((224,224)),
                                transforms.ToTensor()])

# assumes three folders with 60% training, 20% validation and 20% testing samples
train_dataset = datasets.ImageFolder(base_path + 'Training', transform=transform)
val_dataset = datasets.ImageFolder(base_path + 'Validation', transform=transform)
test_dataset = datasets.ImageFolder(base_path + 'Testing', transform=transform)

# Prepare Dataloader
batch_size = 10
num_workers = 1

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,
                                           num_workers=num_workers, shuffle=True)

val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,
                                           num_workers=num_workers, shuffle=True)

test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,
                                           num_workers=num_workers, shuffle=True)

from google.colab import drive
drive.mount('/content/drive')

from torchvision.datasets.folder import IMG_EXTENSIONS
for imgs, labels in train_loader:
  #print(imgs)
  #print(imgs.shape)
  #print(labels)
  for value in labels:
    if value == 0:
      print(True)
  break

# Visualize some sample data
classes = ['F', 'R']

# obtain one batch of training images
dataiter = iter(train_loader)

images, labels = next(dataiter)
images = images.numpy() # convert images to numpy for display

# plot the images in the batch, along with the corresponding labels
fig = plt.figure(figsize=(25, 4))
for idx in np.arange(10):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    plt.imshow(np.transpose(images[idx], (1, 2, 0)))
    ax.set_title(classes[labels[idx]])

torch.manual_seed(10) # set the random seed
from math import floor

class Classifier(nn.Module):
    def __init__(self, kernel_sizes = [3, 2, 3], name = "CNN_Classifier"):
        super(Classifier, self).__init__()
        self.conv1 = nn.Conv2d(3, 4, kernel_sizes[0])
        self.conv2 = nn.Conv2d(4, 5, kernel_sizes[1])
        self.conv3 = nn.Conv2d(5, 8, kernel_sizes[2])

        self.pool = nn.MaxPool2d(2, 2)

        # Computing the correct input size into the Fully Connected Layer
        self.x = floor((224 - kernel_sizes[0] + 1)/2)
        self.y = floor((self.x - kernel_sizes[1] + 1)/2)
        self.z = floor((self.y - kernel_sizes[2] + 1)/2)
        self.FC_input = 8*self.z*self.z

        self.fc1 = nn.Linear(self.FC_input, 10)
        self.fc2 = nn.Linear(10, 1)

        self.name = name

    def forward(self, img):
        x = self.pool(F.relu(self.conv1(img)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = x.view(-1, self.FC_input)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        x = x.squeeze(1)
        return x

def get_model_name(name, batch_size, learning_rate, epoch):
    path = "model_{0}_bs{1}_lr{2}_epoch{3}".format(name, batch_size, learning_rate, epoch)
    return path

use_cuda = True

def get_accuracy(model, data_loader):
    correct = 0
    total = 0
    for imgs, labels in data_loader:

        if use_cuda and torch.cuda.is_available():
          imgs = imgs.cuda()
          labels = labels.cuda()

        output = torch.sigmoid(model(imgs))
        for index in range(output.shape[0]):

          if (output[index] < 0.5 and labels[index] == 0) or (output[index] > 0.5 and labels[index] == 1):
            correct += 1
          else:
            pass
          total += 1
    return correct / total

def train(model, train_dataset, val_dataset, batch_size=20, num_epochs=5, learn_rate=0.001):
    torch.manual_seed(10)

    criterion = nn.CrossEntropyLoss())
    optimizer = optim.Adam(model.parameters(), lr=learn_rate)

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

    train_acc = []
    val_acc = []
    iterations = []


    # training
    print ("Training Started...")
    n = 0
    for epoch in range(num_epochs):
        total_train_loss = 0.0
        total_train_err = 0.0
        total_images = 0
        for imgs, labels in iter(train_loader):

            if use_cuda and torch.cuda.is_available():
              imgs = imgs.cuda()
              labels = labels.cuda()

            out = model(imgs)             # forward pass
            loss = criterion(out, labels.float()) # compute the total loss
            loss.backward()               # backward pass (compute parameter updates)
            optimizer.step()              # make the updates for each parameter
            optimizer.zero_grad()         # a clean up step for PyTorch
            n += 1

        # track accuracy
        n += 1
        iterations.append(n)
        train_acc.append(get_accuracy(model, train_loader))
        val_acc.append(get_accuracy(model, val_loader))

        print(("Epoch {}: Train acc: {} | " + "Validation acc: {}").format(epoch, train_acc[epoch], val_acc[epoch]))

        model_path = get_model_name(model.name, batch_size, learn_rate, epoch)
        torch.save(model.state_dict(), model_path)

    epochs = np.arange(1, num_epochs + 1)

    plt.plot(iterations, train_acc)
    plt.title("Training Curve (Default Parameters)")
    plt.xlabel("Iterations")
    plt.ylabel("Training Accuracy")
    plt.show()

    plt.plot(iterations, val_acc)
    plt.title("Validation Curve (Default Parameters)")
    plt.xlabel("Iterations")
    plt.ylabel("Validation Accuracy")
    plt.show()

    print("Final Training Accuracy: {}".format(train_acc[-1]))
    print("Final Validation Accuracy: {}".format(val_acc[-1]))

# Searching for Hyperparameters
CNN = Classifier()
if use_cuda and torch.cuda.is_available():
  CNN.cuda()

train(CNN, train_dataset, val_dataset)

get_accuracy(CNN, test_loader)

CNN2 = Classifier()
if use_cuda and torch.cuda.is_available():
  CNN2.cuda()

train(CNN2, train_dataset, val_dataset, batch_size = 1)

image = torch.randn(4,4,4)

image

image.max(0, keepdim=True)

image.max(1, keepdim=False)

image.max(2, keepdim=False)